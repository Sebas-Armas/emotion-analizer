{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ce1355",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10f276",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "34ce3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from typing import Tuple, List, Literal, Union\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fdd95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_NUM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "83c9bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfold_splits(\n",
    "        dataset_name: str, \n",
    "        k_folds: int = 10, \n",
    "        split_name: str = \"train\", \n",
    "        limit: int = 100,\n",
    "        **kwargs\n",
    "    ) -> Tuple[List[Dataset], List[Dataset]]:\n",
    "    \"\"\"\n",
    "    Creates dynamic training and validation splits for K-Fold cross-validation using Hugging Face datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset on the Hugging Face Hub.\n",
    "        k_folds (int): Number of folds for cross-validation (e.g., 5 or 10).\n",
    "        split_name (str): Name of the dataset split to apply K-Fold to (typically \"train\").\n",
    "        limit (int): Percentage (0–100) of the dataset to consider when generating the folds.\n",
    "            For example, if limit=50 and k_folds=5, only the first 50% of the split will be used,\n",
    "            divided into 5 equal parts of 10% each.\n",
    "        **kwargs: Additional keyword arguments passed to `load_dataset`\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (train_splits, val_splits) where each is a list of datasets:\n",
    "            - train_splits: List of training subsets (excluding one fold each time).\n",
    "            - val_splits: List of validation subsets (one fold each).\n",
    "    \"\"\"\n",
    "    if limit > 100 or limit <= 0:\n",
    "        raise ValueError(\"The 'limit' parameter must be between 1 and 100.\")        \n",
    "    \n",
    "    val_percentages = [\n",
    "        f\"{split_name}[{i*(limit//k_folds)}%:{(i+1)*(limit//k_folds)}%]\" \n",
    "        for i in range(k_folds)\n",
    "    ]\n",
    "    train_percentages = [\n",
    "        f\"{split_name}[:{i*(limit//k_folds)}%]+{split_name}[{(i+1)*(limit//k_folds)}%:]\"\n",
    "        for i in range(k_folds)\n",
    "    ]\n",
    "    \n",
    "    val_ds = load_dataset(dataset_name, split=val_percentages, **kwargs)\n",
    "    train_ds = load_dataset(dataset_name, split=train_percentages, **kwargs)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "def save_data(\n",
    "    ds: Union[Dataset, List[Dataset]],\n",
    "    name: str,\n",
    "    split: Literal[\"train\", \"test\", \"val\"],\n",
    "    raw: bool,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save a Hugging Face Dataset or list of Datasets to disk.\n",
    "\n",
    "    Parameters:\n",
    "        ds (Dataset or List[Dataset]): The dataset(s) to save.\n",
    "        name (str): Base name for the saved dataset folder(s).\n",
    "        split (str): One of 'train', 'test', or 'val'.\n",
    "        raw (bool): Whether to save under 'raw' or 'processed'.\n",
    "    \"\"\"\n",
    "    base_type = \"raw\" if raw else \"processed\"\n",
    "    root_dir = Path(\"../data\") / base_type\n",
    "\n",
    "    # Validate base directory\n",
    "    if not root_dir.exists():\n",
    "        raise FileNotFoundError(f\"Expected directory does not exist: {root_dir}\")\n",
    "\n",
    "    split_dir = root_dir / split\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save dataset(s)\n",
    "    if isinstance(ds, Dataset):\n",
    "        ds.save_to_disk(split_dir / name)\n",
    "    elif isinstance(ds, list):\n",
    "        for idx, subset in enumerate(ds):\n",
    "            subset.save_to_disk(split_dir / f\"{name}_{idx}\")\n",
    "    else:\n",
    "        raise TypeError(\"ds must be a Dataset or a list of Datasets.\")\n",
    "\n",
    "def load_data(\n",
    "    name: str,\n",
    "    split: Literal[\"train\", \"test\", \"val\"],\n",
    "    raw: bool\n",
    ") -> Union[Dataset, List[Dataset]]:\n",
    "    \"\"\"\n",
    "    Load a saved Hugging Face Dataset or list of Datasets from disk.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): Base name of the saved dataset folder(s).\n",
    "        split (str): One of 'train', 'test', or 'val'.\n",
    "        raw (bool): Whether to load from 'raw' or 'processed'.\n",
    "\n",
    "    Returns:\n",
    "        Dataset or List[Dataset]: The loaded dataset(s).\n",
    "    \"\"\"\n",
    "    base_type = \"raw\" if raw else \"processed\"\n",
    "    root_dir = Path(\"../data\") / base_type\n",
    "\n",
    "    # Validate root directory\n",
    "    if not root_dir.exists():\n",
    "        raise FileNotFoundError(f\"Base directory does not exist: {root_dir}\")\n",
    "\n",
    "    split_dir = root_dir / split\n",
    "    if not split_dir.exists():\n",
    "        raise FileNotFoundError(f\"Split directory does not exist: {split_dir}\")\n",
    "\n",
    "    # Look for matching folders\n",
    "    matching_dirs = sorted(split_dir.glob(f\"{name}*\"))\n",
    "\n",
    "    if not matching_dirs:\n",
    "        raise FileNotFoundError(f\"No dataset directories found for base name '{name}' in {split_dir}\")\n",
    "\n",
    "    # Decide whether it's a single or multiple datasets\n",
    "    if len(matching_dirs) == 1 and matching_dirs[0].name == name:\n",
    "        return load_from_disk(matching_dirs[0])\n",
    "    else:\n",
    "        return [load_from_disk(d) for d in matching_dirs if d.name.startswith(name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b960011",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = create_kfold_splits(\n",
    "    \"google-research-datasets/go_emotions\", \n",
    "    k_folds=CV_NUM, \n",
    "    limit=90,\n",
    "    name='raw',\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "test_ds = load_dataset(\"google-research-datasets/go_emotions\", \n",
    "                       'raw',\n",
    "                       split='train[90%:]',\n",
    "                       trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2153d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776]\n",
      "[8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449]\n",
      "21123\n"
     ]
    }
   ],
   "source": [
    "print([ds.num_rows for ds in train_ds])\n",
    "print([ds.num_rows for ds in val_ds])\n",
    "print(test_ds.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "584f107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 132708.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 147627.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 149387.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 143756.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 141147.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 151855.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 151905.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 157888.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:03<00:00, 53022.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:02<00:00, 98439.41 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 109874.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 132422.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 118976.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 124420.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 126818.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 136439.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 136220.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 141926.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 138821.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 202776/202776 [00:01<00:00, 122568.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 81464.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 79917.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 77009.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 50266.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 68305.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 91154.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 68318.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 76320.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 77721.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 93598.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 73339.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 83054.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 77014.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 85572.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 64178.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 73031.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 76320.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 77718.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 91092.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8449/8449 [00:00<00:00, 71793.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 21123/21123 [00:00<00:00, 116683.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "save_data(train_ds, 'ge_fold', 'train', True)\n",
    "save_data(val_ds, 'ge_fold', 'val', True)\n",
    "save_data(test_ds, 'ge', 'test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b20330f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_data('ge_fold', 'train', True)\n",
    "val_ds = load_data('ge_fold', 'val', True)\n",
    "test_ds = load_data('ge', 'test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09dd4342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776, 202776]\n",
      "[8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449, 8449]\n",
      "21123\n"
     ]
    }
   ],
   "source": [
    "print([ds.num_rows for ds in train_ds])\n",
    "print([ds.num_rows for ds in val_ds])\n",
    "print(test_ds.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036ccee",
   "metadata": {},
   "source": [
    "## Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d498624",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
